{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s_bhandari18/workspace/envs/envrl/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from feature_engine import prepare_train_test, sliding_window, MyDataset, transform_data\n",
    "from config import PARAMConfig, NetworkConfig\n",
    "from networks import ChildNetwork, PolicyNetwork\n",
    "from train import TrainManager\n",
    "from utils import ActionSelection, DataLoader, reward_func, prepare_plot\n",
    "\n",
    "N_EPISODE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Params:\n",
    "    NUM_EPOCHS = 5\n",
    "    ALPHA = 5e-3  # learning rate\n",
    "    BATCH_SIZE = 3  #\n",
    "    HIDDEN_SIZE = 64  # number of hidden nodes we have in our child network\n",
    "    BETA = 0.1  # the entropy bonus multiplier\n",
    "    INPUT_SIZE = 3\n",
    "    ACTION_SPACE = 3\n",
    "    NUM_STEPS = 3  # for 3 params\n",
    "    GAMMA = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 5\n",
    "total_rewards = []\n",
    "writer = SummaryWriter()\n",
    "\n",
    "network_conf = NetworkConfig(\n",
    "    input_size=3, hidden_size=64, num_steps=3, action_space=3, learning_rate=0.001\n",
    ")\n",
    "\n",
    "trainset, valset, testset = prepare_train_test()\n",
    "\n",
    "X_train, y_train = sliding_window(trainset, seq_length)\n",
    "X_val, y_val = sliding_window(valset, seq_length)\n",
    "X_test, y_test = sliding_window(testset, seq_length)\n",
    "train_loader = DataLoader(X_train, y_train)\n",
    "val_loader = DataLoader(X_val, y_val)\n",
    "test_loader = DataLoader(X_test, y_test)\n",
    "\n",
    "episode = 0\n",
    "\n",
    "while episode < N_EPISODE:\n",
    "    initial_state = [[3, 8, 16]]\n",
    "    logit_list = torch.empty(size=(0, network_conf.action_space))\n",
    "    weighted_log_prob_list = torch.empty(size=(0,), dtype=torch.float)\n",
    "    policy_network = PolicyNetwork.from_dict(dict(network_conf._asdict()))\n",
    "\n",
    "    action, log_prob, logits = policy_network.get_action(initial_state)\n",
    "\n",
    "    child_network = ChildNetwork.from_dict(action)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.SGD(child_network.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_manager = TrainManager(\n",
    "        model=child_network, criterion=criterion, optimizer=optimizer\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    train_manager.train(train_loader, val_loader)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    signal = train_manager.avg_validation_loss\n",
    "    reward = reward_func(signal)\n",
    "\n",
    "    weighted_log_prob = log_prob * reward\n",
    "    total_weighted_log_prob = torch.sum(weighted_log_prob).unsqueeze(dim=0)\n",
    "\n",
    "    weighted_log_prob_list = torch.cat(\n",
    "        (weighted_log_prob_list, total_weighted_log_prob), dim=0\n",
    "    )\n",
    "    logit_list = torch.cat((logit_list, logits), dim=0)\n",
    "    # update the controller network\n",
    "    policy_network.update(logit_list, weighted_log_prob_list)\n",
    "\n",
    "    total_rewards.append(reward)\n",
    "\n",
    "    #prepare metrics\n",
    "    current_action = map(str, list(action.values()))\n",
    "    action_str = \"/\".join(current_action)\n",
    "    ActionSelection.update_selection(action_str)\n",
    "    ActionSelection.update_reward(action_str, reward)\n",
    "\n",
    "\n",
    "    #reporting\n",
    "    current_action = f\"Action selection (Hidden size:{action['n_hidden']}, #layers {action['n_layers']}, drop_prob {action['dropout_prob']}).\"\n",
    "    current_run = \"Runs_{}\".format(episode + 1) + \" \" + current_action\n",
    "    counter = 0\n",
    "    for train_loss, val_loss in zip(train_manager.train_losses, train_manager.val_losses):\n",
    "        writer.add_scalars(\n",
    "            current_run, {\"train_loss\": train_loss, \"val_loss\": val_loss}, counter\n",
    "        )\n",
    "        counter += 1\n",
    "\n",
    "    writer.add_scalar(\n",
    "        tag=\"Average Return over {} episodes\".format(N_EPISODE),\n",
    "        scalar_value=np.mean(total_rewards),\n",
    "        global_step=episode,\n",
    "    )\n",
    "\n",
    "    writer.add_scalar(\n",
    "        tag=\"Entropy over time\", scalar_value=policy_network.entropy_mean, global_step=episode\n",
    "    )\n",
    "    writer.add_scalar(\n",
    "        tag=\"Episode runtime\", scalar_value=elapsed, global_step=episode\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Prepare the plot\n",
    "    plot_buf = prepare_plot(ActionSelection.action_selection, \"Action Selection (n_hidden, n_layers, dropout)\")\n",
    "\n",
    "    image = PIL.Image.open(plot_buf)\n",
    "\n",
    "    image = ToTensor()(image)  # .unsqueeze(0)\n",
    "\n",
    "    writer.add_image(\"Image 1\", image, episode)\n",
    "\n",
    "    # Prepare the plot\n",
    "    plot_buf = prepare_plot(\n",
    "        ActionSelection.reward_distribution(), \"Reward Distribution per action selection (n_hidden, n_layers, dropout)\"\n",
    "    )\n",
    "\n",
    "    image = PIL.Image.open(plot_buf)\n",
    "\n",
    "    image = ToTensor()(image)  # .unsqueeze(0)\n",
    "\n",
    "    writer.add_image(\"Image 2\", image, episode)\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
